# Checklist for Creating a DuckDB Schema Map from dbt Project

This document outlines the steps to generate a comprehensive schema map of your dbt project, providing an at-a-glance view of all available data elements. The primary source of truth for this metadata is the `manifest.json` file generated by dbt.

## 1. Prerequisites

- [X] **dbt CLI:** Ensure dbt CLI is installed and configured for your project. Your dbt project appears to be located at `/workspaces/bop-modeling-data-2025/bop_modeling_data/`. (Verified by project structure)
- [X] **Python (Recommended for scripting):** Ensure Python is installed if you plan to parse `manifest.json` programmatically. (Verified by presence of .py files and uv.lock)

## 2. Generate/Update dbt Artifacts

The `manifest.json` file is crucial as it contains all metadata about your dbt project.

- [X] **Navigate to dbt project root:** (Assumed for dbt commands to work)
  ```bash
  cd /workspaces/bop-modeling-data-2025/bop_modeling_data
  ```
- [ ] **Compile your dbt project:**
  ```bash
  dbt compile
  ```
  Alternatively, `dbt docs generate` also performs a compilation.
- [X] **Locate `manifest.json`:** This file will be generated/updated at `target/manifest.json` (i.e., `/workspaces/bop-modeling-data-2025/bop_modeling_data/target/manifest.json`). (Verified by file presence)

## 3. Methods for Extracting Schema Information

Choose one or more methods below based on your needs.

### Method A: Using dbt Docs (Recommended for Interactive Exploration)

dbt Docs provides a user-friendly web interface to explore your project's schema, descriptions, and lineage.

- [ ] **Generate dbt Documentation (if not already done by `dbt compile` with docs generation enabled or `dbt docs generate`):**
  ```bash
  dbt docs generate
  ```
- [ ] **Serve the documentation locally:**
  ```bash
  dbt docs serve
  ```
- [ ] **Access in browser:** Open the URL provided (usually `http://localhost:8000`).
- [ ] **Explore:** Navigate through the "Models", "Sources", and other sections to view schemas, column details (name, type, description), model SQL, and dependencies.

### Method B: Parsing `manifest.json` with a Script (Recommended for Automated Extraction)

This method allows for programmatic extraction and formatting of the schema into your desired output (e.g., Markdown, CSV).

- [X] **Understand `manifest.json` structure:** (Implicitly understood by `schema_map.py`)
    - `nodes`: Contains information about models, seeds, tests, snapshots.
        - Key fields per node: `unique_id`, `resource_type`, `name`, `schema` (target schema), `alias` (actual relation name), `description`, `columns` (object with column name as key and details like `data_type`, `description` as value).
    - `sources`: Contains information about source tables.
        - Key fields per source: `unique_id`, `source_name`, `name` (table name), `schema` (source schema), `description`, `columns`.
- [X] **Write a Python script:** (`schema_map.py` created)
    - [X] Load the JSON from `/workspaces/bop-modeling-data-2025/bop_modeling_data/target/manifest.json`.
    - [X] Iterate through `manifest['nodes']` (filtering for `resource_type` like 'model', 'seed') and `manifest['sources']`.
    - [X] For each node/source, extract:
        - [X] Full unique ID
        - [X] Name and Alias/Table Name
        - [X] Target Schema / Source Schema
        - [X] Description (model/source level)
        - [X] For each column:
            - [X] Column Name
            - [X] Data Type
            - [X] Column Description
    - [X] Output the extracted data into a structured format (YAML).
- [ ] **Execute the script** to generate your schema map.
  ```bash
  python schema_map.py
  ```
- [ ] **Refine `schema_map.py` based on Senior Engineer Feedback:**
    - [ ] **Implement command-line arguments** for input `manifest.json` path and output YAML file path (e.g., using `argparse`).
    - [ ] **Add a comprehensive suite of unit tests** (e.g., using `pytest` and `unittest.mock`).
        - [ ] Test `parse_manifest()` for missing file, valid JSON, invalid JSON.
        - [ ] Test `extract_schema_info()` for empty manifest, models only, sources only, mixed, presence/absence of optional fields, column population, sorting.
        - [ ] Test `save_to_yaml()` for correct `yaml.dump` calls, handling of empty schema_map.
        - [ ] Test `main()` with a sample manifest and verify output YAML.
    - [ ] (Optional) Transition from `print()` to the `logging` module.

### Method C: Using `jq` for Command-Line Extraction

`jq` is a powerful command-line JSON processor for quick queries.

- [ ] **Extract model names:**
  ```bash
  jq -r '.nodes | map(select(.resource_type == "model")) | .[] | .unique_id' /workspaces/bop-modeling-data-2025/bop_modeling_data/target/manifest.json
  ```
- [ ] **Extract columns for a specific model (e.g., `model.bop_modeling_data.dm__npc_counts`):**
  ```bash
  jq -r '.nodes."model.bop_modeling_data.dm__npc_counts".columns | to_entries[] | "\(.key) (\(.value.data_type // "N/A")): \(.value.description // "")"' /workspaces/bop-modeling-data-2025/bop_modeling_data/target/manifest.json
  ```
- [ ] **Combine `jq` queries** to build up the desired schema information, potentially redirecting output to a file.

### Method D: Direct Database Inspection (After `dbt run`)

This method inspects the actual database schema created by dbt. It's good for verification but might lack descriptive metadata from `schema.yml` if not propagated as comments by the adapter.

- [ ] **Run your dbt models:**
  ```bash
  dbt run
  ```
  This will materialize your models as tables/views in your DuckDB database (e.g., `dev.duckdb` or the one specified in your `profiles.yml` for the `bop_modeling_db` profile, likely `bop_modeling_data/bop_modeling_data.duckdb`).
- [ ] **Connect to DuckDB:**
  ```bash
  duckdb /workspaces/bop-modeling-data-2025/dev.duckdb # Or the correct path to your .duckdb file
  ```
- [ ] **Inspect schema using SQL:**
    - Show all tables/views in a specific schema (e.g., `main` or your target schemas):
      ```sql
      SELECT table_name, table_schema FROM information_schema.tables WHERE table_schema = 'your_target_schema' ORDER BY table_name;
      -- Or show all:
      SHOW ALL TABLES;
      ```
    - Describe a specific table/view:
      ```sql
      DESCRIBE your_schema_name.your_table_or_view_name;
      -- Or
      PRAGMA table_info('your_schema_name.your_table_or_view_name');
      ```
    - List columns and types (descriptions from dbt might not be here):
      ```sql
      SELECT column_name, data_type FROM information_schema.columns WHERE table_schema = 'your_schema_name' AND table_name = 'your_table_or_view_name';
      ```

## 4. Consolidating and Presenting the Schema Map

- [X] **Choose a format:** YAML (as implemented by `schema_map.py`).
- [X] **Structure:** For each data asset (model, source, seed): (as implemented by `schema_map.py`)
    - [X] Full Name / Unique ID
    - [X] Type (Model, Source, Seed)
    - [X] Database Schema (e.g., `raw`, `stg`, `mrt`)
    - [X] Table/View Name (as it appears in the database)
    - [X] Overall Description
    - [X] Columns:
        - [X] Column Name
        - [X] Data Type
        - [X] Column Description (from `schema.yml`)
- [ ] **Optional:** Include model dependencies/lineage if helpful.

## 5. Maintenance

- [ ] **Regularly update:** Re-generate the schema map whenever there are changes to your dbt models, sources, or `schema.yml` files.
- [ ] **Automate (Recommended):** If using scripting (Method B), incorporate the script into your development workflow or a CI/CD pipeline to keep the schema map consistently up-to-date.

By following these steps, you can create and maintain a valuable schema map for your dbt project on DuckDB.
